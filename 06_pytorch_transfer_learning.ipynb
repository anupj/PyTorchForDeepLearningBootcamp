{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXMAMsD8VJq0oM6UBCrn+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anupj/PyTorchForDeepLearningBootcamp/blob/main/06_pytorch_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning\n",
        "\n",
        "What is **transfer learning**?\n",
        "Transfer learning is taking the parameters of what one model has learned on another dataset and applying to our own problem.\n",
        "\n",
        "* Pretrained model = foundation models\n",
        "\n",
        "For example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model."
      ],
      "metadata": {
        "id": "VM-h_L06_sCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting Setup"
      ],
      "metadata": {
        "id": "dWZldpcCHFlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo....installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhSSwX1_EvBo",
        "outputId": "dc1cb870-ecc7-47be-f1a0-0ee5bce2e2aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo....installing it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of writing all of the code again, lets download the `going_modular` directory that we created in the previous module."
      ],
      "metadata": {
        "id": "U0UP_bW7H3FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to import the going_modular directory, download it from Github if it doesn't work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] Couldn't find going_modular scripts...downloading them from Github.\")\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoMDykWRHOoJ",
        "outputId": "41e08522-19fc-4d69-ba81-eead4d8e8959"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find going_modular scripts...downloading them from Github.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4356, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 4356 (delta 154), reused 119 (delta 119), pack-reused 4171 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4356/4356), 654.37 MiB | 27.92 MiB/s, done.\n",
            "Resolving deltas: 100% (2583/2583), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9PIfngPMIA8-",
        "outputId": "38b52025-54ce-464e-d14a-ac327899cc1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n",
        "\n",
        "Let's write some code to download the `pizza_steak_sushi.zip` dataset."
      ],
      "metadata": {
        "id": "ZUnTVAA0IA6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path} directory, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Download pizza, steak, sushi data\n",
        "  with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  # Unzip pizza, steak, sushi data\n",
        "  with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "  # Remove .zip file\n",
        "  os.remove(data_path / \"pizza_steak_sushi.zip\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJjelC0OIA31",
        "outputId": "588e0c6d-a548-4f2f-c83e-19edf5bfad5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Setup directory path\n",
        " train_dir = image_path / \"train\"\n",
        " test_dir = image_path / \"test\"\n",
        " train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvfkdxbR9-ak",
        "outputId": "34183c6c-134a-4c74-8198-f4b292583307"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Datasets and Dataloaders\n",
        "\n",
        "We will use `data_setup.py` and the `create_dataloaders()` function we made in `05. PyTorch Going Modular`.\n",
        "\n",
        "There's one thing we have to think about when loading: how to **transform** it?\n",
        "\n",
        "There are two ways to go about this:\n",
        "1. Manually created transforms - you define what transforms you want your data to go through\n",
        "2. Automatically created transforms - the transforms for your data are defined by the model you'd like to use\n",
        "\n",
        "> Important point: when using a pretrained model, it's important that the data (including your custom data) that you pass through it is transformed the same way that the data the model was trained on."
      ],
      "metadata": {
        "id": "6XA7ieD-97CM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "When using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.\n",
        "\n",
        "Prior to torchvision v0.13+, to create a transform for a pretrained model in torchvision.models, the documentation stated:\n",
        "\n",
        "> All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\n",
        "\n",
        "> The images have to be loaded in to a range of `[0, 1]` and then normalized using mean = `[0.485, 0.456, 0.406]` and std = `[0.229, 0.224, 0.225]`.\n",
        "\n",
        ">You can use the following transform to normalize:\n",
        "```\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "```\n",
        "\n",
        "Let's compose a series of `torchvision.transforms` to perform the above steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "7x6Fg6KBIA1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "-dT1f6fMIAy2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got a manually created series of transforms ready to prepare our images, let's create training and testing DataLoaders.\n",
        "\n",
        "We can create these using the `create_dataloaders` function from the `data_setup.py` script we created before.\n",
        "\n",
        "We'll set `batch_size=32` so our model sees mini-batches of 32 samples at a time.\n",
        "\n",
        "And we can transform our images using the transform pipeline we created above by setting `transform=manual_transforms`."
      ],
      "metadata": {
        "id": "LmgnRAE3QEc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                        test_dir=test_dir,                                                           transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize thems\n",
        "batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "eJurULW_IAtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a570b639-bc7b-445f-d0a0-3622cc6a740f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7b20f3277220>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7b20f3276ef0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Creating a transform for torchvision.models (auto creation)\n",
        "As previously stated, when using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.\n",
        "\n",
        "Above we saw how to manually create a transform for a pretrained model.\n",
        "\n",
        "But as of `torchvision v0.13+`, an automatic transform creation feature has been added.\n",
        "\n",
        "When you setup a model from `torchvision.models` and select the pretrained model weights you'd like to use, for example, say we'd like to use:\n",
        "```\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "```\n",
        "Where,\n",
        "\n",
        "`EfficientNet_B0_Weights` is the model architecture weights we'd like to use (there are many different model architecture options in torchvision.models).\n",
        "\n",
        "`DEFAULT` means the best available weights (the best performance in ImageNet).\n",
        "\n",
        ">Note: Depending on the model architecture you choose, you may also see other options such as `IMAGENET_V1` and `IMAGENET_V2` where generally the higher version number the better. Though if you want the best available, `DEFAULT` is the easiest option. See the `torchvision.models` documentation for more.\n",
        "\n"
      ],
      "metadata": {
        "id": "lHQk5xYwIARR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of pre-trained models\n",
        "# .DEFAULT = best available weights from pretraining on ImageNet\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGjnBj4UU_8_",
        "outputId": "92f26dd2-8a22-47dc-b327-b7fcd369d725"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet_B0_Weights.IMAGENET1K_V1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms  = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuPBYOGtW11r",
        "outputId": "197450ea-497f-4b2f-ada7-c5b8e369cce2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_-Ftr4noU2IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders using automatic transforms\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir = train_dir,test_dir = test_dir, transform=auto_transforms, batch_size=32)\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYw6W6EBXCul",
        "outputId": "097824f3-7e0a-4503-85f1-c491e832f014"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7b20f3274b20>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7b20f3276200>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "There are various places to get a pretrained model, such as:\n",
        "1. PyTorch domain libraries\n",
        "2. Libraries like `timm` (torch image models)\n",
        "3. HuggingFace Hub\n",
        "4. Paperswithcode\n",
        "\n",
        "But how do you choose a model?\n",
        "\n",
        "*Experiment! Experiment! Experiment!*\n",
        "\n",
        "Four things to consider:\n",
        "1. Speed - how fast does it run?\n",
        "2. Size - how big is the model?\n",
        "3. Performance - how well does it perform on your chosen problem space?\n",
        "4. Deployment target -\n",
        "  - Is it on device? (like a self-driving car)\n",
        "  - or does it live on a server?\n",
        "\n",
        "For our case, EffNetB0 is one of our best options in terms of performance vs size."
      ],
      "metadata": {
        "id": "JABzxVdYV1Es"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNg548IKV1j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhY5JeRiV15Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_-apwRHV2B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wh5CKtGSV2KW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}