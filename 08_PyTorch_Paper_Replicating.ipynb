{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnP5PMKtLX11588uxzdSFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anupj/PyTorchForDeepLearningBootcamp/blob/main/08_PyTorch_Paper_Replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08. Milestone Project 2: PyTorch Paper Replicating\n",
        "\n",
        "The goal of this project is to turn a ML research paper into code.\n",
        "\n",
        "In this notebook, we'll replicate the Vision Transformer (ViT) architecture [paper](https://arxiv.org/abs/2010.11929).\n"
      ],
      "metadata": {
        "id": "IMC__c6rcmTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting Setup\n",
        "\n",
        "Let's import code we've previously written + required libraries like `torchinfo`."
      ],
      "metadata": {
        "id": "42aNlt11dN7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9zGd_6Udh23",
        "outputId": "b36210c3-3b73-4342-9684-69bc6e6a0cf5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "0.20.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKZF1ebke7aZ",
        "outputId": "fe08f721-70f3-4fe6-ef6e-42b71a0daa98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4356, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "ERyBbIeSfK77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n",
        "\n",
        "Since we're continuing on with FoodVision Mini, let's download the pizza, steak and sushi image dataset we've been using.\n",
        "\n",
        "To do so we can use the `download_data()` function from `helper_functions.py` that we created before."
      ],
      "metadata": {
        "id": "Arxk0k0TfTef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "VnZPMG92j50M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "6ZVofS67kBz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Let's start by creating `DataLoaders` using `create_dataloaders()` function we've written before and imported in our `going_modular` folder.\n",
        "\n",
        "First we'll create a *transform* to *prepare* our images. How will we determine the image size?\n",
        "\n",
        "This is where we can refer to the ViT paper to see if they've provided this information. And indeed in **Table 3** - `Hyperparameters for Training` we can see that the training resolution is **224**, and the batch size is **4096**.\n",
        "\n",
        "Given our limited Google Colab resources, we won't be using that big of a batch size, instead we'll stick with batch size of 32. Onwards."
      ],
      "metadata": {
        "id": "wr4RvumlkHqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Prepare transforms for images"
      ],
      "metadata": {
        "id": "Ne8P4VkwmbIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image size hyperparameter\n",
        "IMG_SIZE = 224 # as mentioned in the ViT paper\n",
        "\n",
        "from torchvision import transforms\n",
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "# Since we are not using transfer learning to begin with, we'll create our transform manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "print(f\"Manually created transforms: {manual_transforms}\")\n"
      ],
      "metadata": {
        "id": "0eFx49iimdBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Turn images into `DataLoader's`\n",
        "\n",
        "As mentioned above, we'll keep our batch size as 32, and we'll set `pin_memory=True`.\n",
        "\n",
        "> **Note**: We're using the pin_memory=True parameter in the `create_dataloaders()` function to speed up computation. `pin_memory=True` avoids unnecessary copying of memory between the CPU and GPU memory by \"pinning\" examples that have been seen before. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). However, setting `pin_memory=True` doesn't always improve performance (this is another one of those we're scenarios in machine learning where some things work sometimes and don't other times), so best to *experiment, experiment, experiment*."
      ],
      "metadata": {
        "id": "KW7aF1YCneoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "len(train_dataloader), len(test_dataloader), class_names"
      ],
      "metadata": {
        "id": "nEQEeUBMoB4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Visualize a single image\n",
        "\n"
      ],
      "metadata": {
        "id": "gkhWshS-o0VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image and label from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the single image and label shape\n",
        "print(f\"image.shape: {image.shape}, label: {label}\") # label is a scalar\n",
        "\n",
        "# Plot the image with mathplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# permute the image because matplot lib prefers (height, width, color_channels) whereas pytorch prefers (color_channels, height, width)\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "7ai46K0lo42q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Replicating the ViT paper: an overview\n",
        "\n",
        "Before we write any more code, let's discuss what we're doing.\n",
        "\n",
        "We'd like to replicate the ViT paper for our own problem, FoodVision Mini.\n",
        "\n",
        "So our model inputs are: images of pizza, steak and sushi.\n",
        "\n",
        "And our ideal model outputs are: predicted labels of pizza, steak or sushi.\n",
        "\n",
        "No different to what we've been doing throughout the previous sections.\n",
        "\n",
        "The question is: how do we go from our inputs to the desired outputs?\n"
      ],
      "metadata": {
        "id": "jzsmWGaVmW0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 3.1. Inputs and outputs, layers and blocks\n",
        "\n",
        "ViT is a deep learning neural network architecture.\n",
        "\n",
        "And any neural network architecture is generally comprised of **layers**.\n",
        "\n",
        "And a collection of layers is often referred to as a **block**.\n",
        "\n",
        "And stacking many blocks together is what gives us the *whole architecture*.\n",
        "\n",
        "A *layer* takes an **input** (say an image tensor), performs some kind of function on it (for example what's in the layer's `forward()` method) and then returns an **output**.\n",
        "\n",
        "So if a single layer takes an input and gives an output, then a collection of layers or a block also takes an input and gives an output.\n",
        "\n",
        "Let's make this concrete:\n",
        "* **Layer** - takes an input, performs a function on it, returns an output.\n",
        "* **Block** - a collection of layers, takes an input, performs a series of functions on it, returns an output.\n",
        "* **Architecture (or model)** - a collection of blocks, takes an input, performs a series of functions on it, returns an output.\n",
        "\n",
        "This ideology is what we're going to be using to replicate the ViT paper.\n",
        "\n",
        "We're going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.\n",
        "\n",
        "The reason we do this is because looking at a whole research paper can be intimidating.\n",
        "\n",
        "So for a better understanding, we'll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model."
      ],
      "metadata": {
        "id": "oG_6GOnJGtUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Getting specific: What's ViT made of?\n",
        "\n",
        "There are many little details about the ViT model sprinkled throughout the paper.\n",
        "\n",
        "Finding them all is like one big treasure hunt!\n",
        "\n",
        "However, the main three resources we'll be looking at for the architecture design are:\n",
        "\n",
        "* **Figure 1** - This gives an overview of the model in a graphical sense, you could almost recreate the architecture with this figure alone.\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/images/08-vit-paper-figure-1-architecture-overview.png)\n",
        "* **Four equations in section 3.1** - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/images/08-vit-paper-four-equations.png)\n",
        "* **Table 1** - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We'll be focused on the smallest version, ViT-Base.\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/images/08-vit-paper-table-1.png)\n",
        "\n",
        "\n",
        "#### 3.2.1. Exploring Figure 1\n",
        "\n",
        "Let's start by going through Figure 1 of the ViT Paper.\n",
        "\n",
        "The main things we'll be paying attention to are:\n",
        "\n",
        "1. **Layers** - takes an input, performs an operation or function on the input, produces an output.\n",
        "2. **Blocks** - a collection of layers, which in turn also takes an input and produces an output.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs.png)\n",
        "*Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.*\n",
        "\n",
        "The ViT architecture is comprised of several stages:\n",
        "\n",
        "* **Patch + Position Embedding (inputs)** - Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.\n",
        "\n",
        "* **Linear projection of flattened patches (Embedded Patches)** - The image patches get turned into an embedding, the benefit of using an embedding rather than just the image values is that an embedding is a *learnable representation (typically in the form of a vector)* of the image that can improve with training.\n",
        "\n",
        "* **Norm** - This is short for **Layer Normalization** or **LayerNorm**, a technique for *regularizing* (reducing overfitting) a neural network, you can use **LayerNorm** via the PyTorch layer `torch.nn.LayerNorm()`.\n",
        "\n",
        "* **Multi-Head Attention** - This is a **Multi-Headed Self-Attention layer** or \"MSA\" for short. You can create an MSA layer via the PyTorch layer `torch.nn.MultiheadAttention()`.\n",
        "\n",
        "* **MLP (or Multilayer perceptron)** - A MLP can often refer to any collection of feedforward layers (or in PyTorch's case, a collection of layers with a `forward()` method). In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two `torch.nn.Linear()` layers with a `torch.nn.GELU()` non-linearity activation in between them (section 3.1) and a `torch.nn.Dropout()` layer after each (Appendix B.1).\n",
        "\n",
        "* **Transformer Encoder** - The **Transformer Encoder**, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the `+` symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.\n",
        "\n",
        "* **MLP Head** - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block.\n",
        "\n",
        "Let's take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs-food-mini.png)\n",
        "*Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class \"pizza\" is returned.*\n",
        "\n",
        "\n",
        "#### 3.2.2 Exploring the Four Equations\n",
        "\n",
        "The next main part(s) of the ViT paper we're going to look at are the four equations in section 3.1.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png)\n",
        "\n",
        "**These four equations represent the math behind the four major parts of the ViT architecture.**\n",
        "\n",
        "\n",
        "Let's map these descriptions to the ViT architecture in Figure 1.\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-mapping-the-four-equations-to-figure-1.png)\n",
        "\n",
        "How about we break down each equation further (it will be our goal to recreate these with code)?\n",
        "\n",
        "In all equations (except equation 4), \"$\\mathbf{z}$\" is the raw output of a particular layer:\n",
        "\n",
        "1. $\\mathbf{z}_{0}$ is \"z zero\" (this is the output of the initial patch embedding layer).\n",
        "2. $\\mathbf{z}_{\\ell}^{\\prime}$ is \"z of a particular layer prime\" (or an intermediary value of z).\n",
        "3. $\\mathbf{z}_{\\ell}$ is \"z of a particular layer\".\n",
        "\n",
        "And $\\mathbf{y}$ is the overall output of the architecture.\n",
        "\n",
        "#### 3.2.3 Equation 1 overview\n",
        "\n",
        "$$ \\begin{aligned} \\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$\n",
        "\n",
        "This equation deals with the class token, patch embedding and position embedding ($\\mathbf{E}$ is for embedding) of the input image.\n",
        "\n",
        "In vector form, the embedding might look something like:\n",
        "\n",
        "```\n",
        "x_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n",
        "```\n",
        "Where each of the elements in the vector is learnable (their `requires_grad=True`).\n",
        "\n",
        "#### 3.2.4 Equation 2 overview\n",
        "$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L \\end{aligned} $$\n",
        "\n",
        "This says that for every layer from $1$ through to $L$ (the total number of layers), there's a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).\n",
        "\n",
        "The addition on the end is the equivalent of adding the input to the output and forming a [skip/residual connection](https://paperswithcode.com/method/residual-connection).\n",
        "\n",
        "We'll call this layer the \"MSA block\".\n",
        "\n",
        "In pseudocode, this might look like:\n",
        "```\n",
        "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "```\n",
        "Notice the skip connection on the end (adding the input of the layers to the output of the layers).\n",
        "\n",
        "\n",
        "#### 3.2.5 Equation 3 overview\n",
        "$$ \\begin{aligned} \\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\\\ \\end{aligned} $$\n",
        "\n",
        "This says that for every layer from $1$ through to $L$ (the total number of layers), there's also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).\n",
        "\n",
        "The addition on the end is showing the presence of a skip/residual connection.\n",
        "\n",
        "We'll call this layer the \"MLP block\".\n",
        "\n",
        "In pseudocode, this might look like:\n",
        "```\n",
        "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
        "```\n",
        "Notice the skip connection on the end (adding the input of the layers to the output of the layers).\n",
        "\n",
        "#### 3.2.6 Equation 4 overview\n",
        "\n",
        "$$ \\begin{aligned} \\mathbf{y} &=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & & \\end{aligned} $$\n",
        "\n",
        "This says for the last layer $L$, the output $y$ is the 0 index token of $z$ wrapped in a LayerNorm layer (LN).\n",
        "\n",
        "Or in our case, the 0 index of `x_output_MLP_block`:\n",
        "\n",
        "```\n",
        "y = Linear_layer(LN_layer(x_output_MLP_block[0]))\n",
        "```\n",
        "\n",
        "#### 3.2.7 Exploring Table 1\n",
        "\n",
        "The final piece of the ViT architecture puzzle we'll focus on (for now) is Table 1.\n",
        "\n",
        "| Model      | Layers | Hidden size $D$ | MLP size | Heads | Params |\n",
        "|-----------|--------|-----------------|----------|-------|--------|\n",
        "| ViT-Base\t| 12\t   |768\t             |3072      |\t12\t  | $86M$  |\n",
        "| ViT-Large\t| 24\t   | 1024\t           | 4096\t    | 16    | $307M$ |\n",
        "| ViT-Huge\t| 32\t   | 1280\t           | 5120\t    | 16    |$632M$  |\n",
        "*Table 1: Details of Vision Transformer model variants. Source: ViT paper.*\n",
        "\n",
        "This table showcasing the various hyperparameters of each of the ViT architectures.\n",
        "\n",
        "You can see the numbers gradually increase from ViT-Base to ViT-Huge.\n",
        "\n",
        "We're going to focus on replicating ViT-Base (start small and scale up when necessary) but we'll be writing code that could easily scale up to the larger variants.\n",
        "\n",
        "Breaking the hyperparameters down:\n",
        "\n",
        "* **Layers** - How many Transformer Encoder blocks are there? (each of these will contain a MSA block and MLP block)\n",
        "* **Hidden size $D$** - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more computation.\n",
        "* **MLP size** - What are the number of hidden units in the MLP layers?\n",
        "* **Heads** - How many heads are there in the Multi-Head Attention layers?\n",
        "* **Params** - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more computation. You'll notice even ViT-Base has far more parameters than any other model we've used so far.\n",
        "\n",
        "We'll use these values as the hyperparameter settings for our ViT architecture."
      ],
      "metadata": {
        "id": "4WvrIhFMnVHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Equation 1: Split image input into patches and creating the class, position, and patch embedding\n",
        "\n",
        "We'll start with the **patch embedding**.\n",
        "\n",
        "This means we'll be turning our input images in *a sequence of patches and then embedding those patches*.\n",
        "\n",
        "Recall that an embedding is a **learnable representation** of some form and is often a vector.\n",
        "\n",
        "The term *learnable* is important because this means the numerical representation of an input image (that the model sees) can be improved over time.\n",
        "\n",
        "We'll begin by following the opening paragraph of section 3.1 of the ViT paper\n",
        ">The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "And size we're dealing with image shapes, let's keep in mind the line from Table 3 of the ViT paper:\n",
        "\n",
        "> Training resolution is 224.\n",
        "\n",
        "Let's break down the text above.\n",
        "\n",
        "- $D$ is the size of the patch embeddings, different values for $D$ for various sized ViT models can be found in Table 1.\n",
        "- The image starts as 2D with size ${H \\times W \\times C}$.\n",
        "  - $(H, W)$ is the resolution of the original image (height, width).\n",
        "  - $C$ is the number of channels.\n",
        "- The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.\n",
        "  - $(P, P)$ is the resolution of each image patch (patch size).\n",
        "  - $N=H W / P^{2}$ is the resulting number of patches, which also serves as the input sequence length for the Transformer.\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-annotated.png)\n"
      ],
      "metadata": {
        "id": "v7EMXx4WmVDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 4.1 Calculating patch embedding input and output shapes by hand\n",
        "\n",
        "To do so, let's create some variables to mimic each of the terms (such as $H$, $W$ etc) above.\n",
        "\n",
        "We'll use a patch size ($P$) of 16 since it's the best performing version of ViT-Base uses (see column \"ViT-B/16\" of Table 5 in the ViT paper for more).\n",
        "\n"
      ],
      "metadata": {
        "id": "wZzp2P23LbTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example values\n",
        "height = 224 # H (\"The training resolution is 224\")\n",
        "width = 224 # W\n",
        "color_channels = 3 # C\n",
        "patch_size = 16 # P\n",
        "\n",
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1FNd7zBLl9_",
        "outputId": "fc10ef7c-648b-4b36-bbeb-9983eac694cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196\n"
          ]
        }
      ]
    }
  ]
}